{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import preprocessor as p\n",
    "import string\n",
    "import re\n",
    "from nltk import TweetTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_tweets = pd.read_csv('Desktop/full_tweets.csv', usecols = [0,5], names = ['label','tweet'], encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_tweets['label'] = larger_tweets['label'].map({0:1, 2:0, 4:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.NUMBER)\n",
    "larger_tweets['tweet_cleaned'] = larger_tweets['tweet'].apply(lambda x : p.clean(x))\n",
    "larger_tweets['tweet_cleaned'] = larger_tweets['tweet_cleaned'].apply(lambda x : re.sub(r'#', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "larger_tweets['tokenized_tweets'] = larger_tweets['tweet_cleaned'].apply(\n",
    "    lambda x : tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_lst = {}\n",
    "for dim in tqdm([25,50,100,200]):\n",
    "    glove_file = 'glove.twitter.27B.' + str(dim) + 'd.txt'\n",
    "    glove = open('Desktop/glove_embeddings/' + glove_file)\n",
    "    emb_dict = {}\n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        emb_dict[word] = vector\n",
    "    dic_lst[str(dim)] = emb_dict\n",
    "    glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_lst['25']['booty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(larger_tweets['tokenized_tweets'], larger_tweets['label'],\n",
    "                                                   test_size = .25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_vec_generator(data, vectors, vec_length):\n",
    "    vocab = vectors.keys()\n",
    "    dense_feature_lst = []\n",
    "    for tweet in tqdm(data.values):\n",
    "        tot_vec = np.zeros((vec_length,))\n",
    "        words = [word for word in tweet if word in vocab]\n",
    "        if len(words) == 0:\n",
    "            dense_feature_lst.append(tot_vec)\n",
    "            continue\n",
    "        for word in words:\n",
    "            tot_vec += vectors[word]\n",
    "        tot_vec = tot_vec / len(words)\n",
    "        dense_feature_lst.append(tot_vec)\n",
    "    return np.array(dense_feature_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feats_25 = word_vec_generator(X_train, dic_lst['25'], 25)\n",
    "X_train_feats_50 = word_vec_generator(X_train, dic_lst['50'], 50)\n",
    "X_train_feats_100 = word_vec_generator(X_train, dic_lst['100'], 100)\n",
    "X_train_feats_200 = word_vec_generator(X_train, dic_lst['200'], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_feats_25 = word_vec_generator(X_test, dic_lst['25'], 25)\n",
    "X_test_feats_50 = word_vec_generator(X_test, dic_lst['50'], 50)\n",
    "X_test_feats_100 = word_vec_generator(X_test, dic_lst['100'], 100)\n",
    "X_test_feats_200 = word_vec_generator(X_test, dic_lst['200'], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lst = [X_train_feats_25, X_train_feats_50, X_train_feats_100, X_train_feats_200]\n",
    "test_lst = [X_test_feats_25,X_test_feats_50,X_test_feats_100,X_test_feats_200]\n",
    "scaled_train_feats = []\n",
    "scaled_test_feats = []\n",
    "for feats in zip(train_lst,test_lst):\n",
    "    scaler = StandardScaler()\n",
    "    fitted = scaler.fit(feats[0])\n",
    "    scaled_train_feats.append(fitted.transform(feats[0]))\n",
    "    scaled_test_feats.append(fitted.transform(feats[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'penalty': ['l1', 'l2'], 'C': [.1,1,10,100]}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "results = []\n",
    "for feat_set in [X_train_feats_25[:10000], X_train_feats_50[:10000], X_train_feats_100[:10000], X_train_feats_200[:10000]]:\n",
    "    search = GridSearchCV(logreg, params, scoring = scores, refit = False)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    results.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['25','50','100','200']:\n",
    "    feats.append([feat_length]*8)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in results:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_lr = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_lr.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric_df_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'penalty': ['l1', 'l2'], 'C': [.1,1,10,100]}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "scaled_results = []\n",
    "for feat_set in [scaled_train_feats[0][:10000], scaled_train_feats[1][:10000], scaled_train_feats[2][:10000], scaled_train_feats[3][:10000]]:\n",
    "    search = GridSearchCV(logreg, params, scoring = scores, refit = False)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    scaled_results.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['25','50','100','200']:\n",
    "    feats.append([feat_length]*8)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in scaled_results:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_scaled_lr = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_scaled_lr.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric_df_scaled_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': [500,1000], 'max_features': ['auto','log2']}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "rf = RandomForestClassifier()\n",
    "results_rf = []\n",
    "for feat_set in [X_train_feats_25[:10000], X_train_feats_50[:10000], X_train_feats_100[:10000], X_train_feats_200[:10000]]:\n",
    "    search = GridSearchCV(rf, params, scoring = scores, refit = False, n_jobs=3)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    results_rf.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['25','50','100','200']:\n",
    "    feats.append([feat_length]*4)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in results_rf:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_rf = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_rf.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric_df_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'n_estimators': [10,100,250], 'max_depth':[10,50], 'eta':[.1,.3,.5]}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "xgb = XGBClassifier(n_jobs=3)\n",
    "results_xgb = []\n",
    "for feat_set in [X_train_feats_25[:10000], X_train_feats_50[:10000], X_train_feats_100[:10000], X_train_feats_200[:10000]]:\n",
    "    search = GridSearchCV(xgb, params, scoring = scores, refit = False, n_jobs=3)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    results_xgb.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['25','50','100','200']:\n",
    "    feats.append([feat_length]*18)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in results_xgb:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_xgb = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_xgb.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric_df_xgb.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [1200,12000,120000,1200000]\n",
    "lr_models = []\n",
    "lr_preds = {}\n",
    "for size in tqdm(training_sizes):\n",
    "    logreg = LogisticRegression(C=1, penalty='l1', solver='saga')\n",
    "    logreg.fit(X_train_feats_200[:size], y_train[:size])\n",
    "    lr_models.append(logreg)\n",
    "    size_preds = logreg.predict(X_test_feats_200[:size])\n",
    "    lr_preds[size] = size_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_size, pred_values in lr_preds.items():\n",
    "    acc = accuracy_score(pred_values, y_test[:train_size])\n",
    "    prec = precision_score(pred_values, y_test[:train_size])\n",
    "    rec = recall_score(pred_values, y_test[:train_size])\n",
    "    f1 = f1_score(pred_values, y_test[:train_size])\n",
    "    print(train_size)\n",
    "    print('accuracy:' + str(acc))\n",
    "    print('precision:' + str(prec))\n",
    "    print('recall:' + str(rec))\n",
    "    print('f1 score:' + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "training_sizes = [1200,12000,120000,500000]\n",
    "models = []\n",
    "xgb_preds = {}\n",
    "for size in tqdm(training_sizes):\n",
    "    xgb = XGBClassifier(max_depth=50, n_estimators = 250, eta = .5, n_jobs=3)\n",
    "    xgb.fit(X_train_feats_200[:size], y_train[:size])\n",
    "    models.append(xgb)\n",
    "    size_preds = xgb.predict(X_test_feats_200[:size])\n",
    "    xgb_preds[size] = size_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_size, pred_values in xgb_preds.items():\n",
    "    acc = accuracy_score(pred_values, y_test[:train_size])\n",
    "    prec = precision_score(pred_values, y_test[:train_size])\n",
    "    rec = recall_score(pred_values, y_test[:train_size])\n",
    "    f1 = f1_score(pred_values, y_test[:train_size])\n",
    "    print(train_size)\n",
    "    print('accuracy:' + str(acc))\n",
    "    print('precision:' + str(prec))\n",
    "    print('recall:' + str(rec))\n",
    "    print('f1 score:' + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = [1200,12000,120000,1200000,1200,12000,120000,500000]\n",
    "models = ['Logistic Regression','Logistic Regression','Logistic Regression','Logistic Regression',\n",
    "          'XGB Classifier','XGB Classifier','XGB Classifier','XGB Classifier']\n",
    "acc = [72.9,75.4,76.0,76.0,69.8,73.2,75.5,76.2]\n",
    "prec = [72.8,73.8,75.3,75.5,67.3,72.2,75.6,76.5]\n",
    "rec = [73.0,76.2,76.3,76.2,70.9,73.5,75.3,76.0]\n",
    "f1 = [72.9,75.0,75.8,75.8,69.1,72.9,75.5,76.3]\n",
    "\n",
    "glove_models = pd.DataFrame()\n",
    "glove_models['Model'] = models\n",
    "glove_models['Training Size'] = train_size\n",
    "glove_models['Accuracy'] = acc\n",
    "glove_models['Precision'] = prec\n",
    "glove_models['Recall'] = rec\n",
    "glove_models['F1 Score'] = f1\n",
    "\n",
    "acc_glove = alt.Chart(glove_models).mark_line().encode(\n",
    "    x=alt.X('Training Size',scale=alt.Scale(type='log')),\n",
    "    y=alt.Y('Accuracy',scale=alt.Scale(domain=[65,80])),\n",
    "    color = 'Model'\n",
    "    ).properties(title='GloVE Embeddings: Accuracy vs. Training Size')\n",
    "\n",
    "acc_glove.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "logreg_preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_report = classification_report(y_test, lr_preds[1200000], output_dict=True)\n",
    "logreg_report = pd.DataFrame(logreg_report).T\n",
    "logreg_report = logreg_report.rename(index={'1':'Negative (0)', '0':'Postive/Neutral (4)'})\n",
    "logreg_report[['precision','recall','f1-score']] = logreg_report[['precision','recall','f1-score']]*100\n",
    "logreg_report = logreg_report.round(1)\n",
    "logreg_report.iloc[2,:2] = ''\n",
    "logreg_report.iloc[2,3] = ''\n",
    "logreg_report.index.name = 'Logistic Regression'\n",
    "logreg_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbreport = classification_report(y_test, xgb_preds[500000], output_dict=True)\n",
    "xgbreport = pd.DataFrame(xgbreport).T\n",
    "xgbreport = xgbreport.rename(index={'1':'Negative (0)', '0':'Postive/Neutral (4)'})\n",
    "xgbreport[['precision','recall','f1-score']] = xgbreport[['precision','recall','f1-score']]*100\n",
    "xgbreport = xgbreport.round(1)\n",
    "xgbreport.iloc[2,:2] = ''\n",
    "xgbreport.iloc[2,3] = ''\n",
    "xgbreport.index.name = 'XGBoost'\n",
    "xgbreport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
