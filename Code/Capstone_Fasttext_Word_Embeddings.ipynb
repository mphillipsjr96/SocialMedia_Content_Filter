{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import preprocessor as p\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_tweets = pd.read_csv('Desktop/full_tweets.csv', usecols = [0,5], names = ['label','tweet'], encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_tweets['label'] = larger_tweets['label'].map({0:1, 2:0, 4:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.NUMBER)\n",
    "larger_tweets['tweet_cleaned'] = larger_tweets['tweet'].apply(lambda x : p.clean(x))\n",
    "larger_tweets['tweet_cleaned'] = larger_tweets['tweet_cleaned'].apply(lambda x : re.sub(r'#', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "larger_tweets['tokenized_tweets'] = larger_tweets['tweet_cleaned'].apply(\n",
    "    lambda x : tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(larger_tweets['tokenized_tweets'], larger_tweets['label'],\n",
    "                                                   test_size = .25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vec_generator_ft(data, vectors, vec_size):\n",
    "    dense_feature_lst = []\n",
    "    for tweet in tqdm(data.values):\n",
    "        tot_vec = np.zeros((vec_size,))\n",
    "        if len(tweet) == 0:\n",
    "            dense_feature_lst.append(tot_vec)\n",
    "            continue\n",
    "        for word in tweet:\n",
    "            tot_vec += vectors.get_word_vector(word)\n",
    "        tot_vec = tot_vec / len(tweet)\n",
    "        dense_feature_lst.append(tot_vec)\n",
    "    return np.array(dense_feature_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "ft_eng = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_300 = word_vec_generator_ft(X_train, ft_eng, 300)\n",
    "X_test_300 = word_vec_generator_ft(X_test, ft_eng, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.util.reduce_model(ft_eng, 200)\n",
    "X_train_200 = word_vec_generator_ft(X_train, ft_eng, 200)\n",
    "X_test_200 = word_vec_generator_ft(X_test, ft_eng, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.util.reduce_model(ft_eng, 100)\n",
    "X_train_100 = word_vec_generator_ft(X_train, ft_eng, 100)\n",
    "X_test_100 = word_vec_generator_ft(X_test, ft_eng, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.util.reduce_model(ft_eng, 50)\n",
    "X_train_50 = word_vec_generator_ft(X_train, ft_eng, 50)\n",
    "X_test_50 = word_vec_generator_ft(X_test, ft_eng, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lst = [X_train_50, X_train_100, X_train_200, X_train_300]\n",
    "test_lst = [X_test_50, X_test_100, X_test_200, X_test_300]\n",
    "scaled_train_feats = []\n",
    "scaled_test_feats = []\n",
    "for feats in zip(train_lst,test_lst):\n",
    "    scaler = StandardScaler()\n",
    "    fitted = scaler.fit(feats[0])\n",
    "    scaled_train_feats.append(fitted.transform(feats[0]))\n",
    "    scaled_test_feats.append(fitted.transform(feats[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'penalty': ['l1', 'l2'], 'C': [.1,1,10,100]}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "results = []\n",
    "for feat_set in [X_train_50[:10000], X_train_100[:10000], X_train_200[:10000], X_train_300[:10000]]:\n",
    "    search = GridSearchCV(logreg, params, scoring = scores, refit = False)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    results.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['25','50','100','200']:\n",
    "    feats.append([feat_length]*8)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in results:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_lr = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_lr.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric_df_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'penalty': ['l1', 'l2'], 'C': [.1,1,10,100]}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "scaled_results = []\n",
    "for feat_set in [scaled_train_feats[0][:10000], scaled_train_feats[1][:10000], scaled_train_feats[2][:10000], scaled_train_feats[3][:10000]]:\n",
    "    search = GridSearchCV(logreg, params, scoring = scores, refit = False)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    scaled_results.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['25','50','100','200']:\n",
    "    feats.append([feat_length]*8)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in scaled_results:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_lr = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_lr.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_metric_df_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'n_estimators': [500,1000], 'max_features': ['auto','log2']}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "rf = RandomForestClassifier()\n",
    "results_rf = []\n",
    "for feat_set in [X_train_50[:10000], X_train_100[:10000], X_train_200[:10000], X_train_300[:10000]]:\n",
    "    search = GridSearchCV(rf, params, scoring = scores, refit = False, n_jobs=3)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    results_rf.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['50','100','200','300']:\n",
    "    feats.append([feat_length]*4)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in results_rf:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_rf = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_rf.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric_df_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'n_estimators': [10,100,250], 'max_depth':[10,50], 'eta':[.1,.3,.5]}\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "xgb = XGBClassifier(n_jobs=3)\n",
    "results_xgb = []\n",
    "for feat_set in [X_train_50[:10000], X_train_100[:10000], X_train_200[:10000], X_train_300[:10000]]:\n",
    "    search = GridSearchCV(xgb, params, scoring = scores, refit = False, n_jobs=3)\n",
    "    search.fit(feat_set, y_train[:10000])\n",
    "    results_xgb.append(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for feat_length in ['50','100','200','300']:\n",
    "    feats.append([feat_length]*18)\n",
    "feats = [val for lst in feats for val in lst]\n",
    "df_lst = []\n",
    "for result in results_xgb:\n",
    "    dic = {}\n",
    "    dic['params'] = result['params']\n",
    "    dic['accuracy'] = result['mean_test_accuracy']\n",
    "    dic['precision'] = result['mean_test_precision']\n",
    "    dic['recall'] = result['mean_test_recall']\n",
    "    dic['f1_score'] = result['mean_test_f1']\n",
    "    df_lst.append(pd.DataFrame(dic))\n",
    "full_metric_df_xgb = pd.concat(df_lst, axis = 0)\n",
    "full_metric_df_xgb.insert(0,'word_vector_length',feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric_df_xgb.tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
